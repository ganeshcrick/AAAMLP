{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd004812d5341259191e05fd495720a6add0c0e594cd649530362bde92e927c06cf",
   "display_name": "Python 3.8.5 64-bit ('ML': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-1-41d87f4e5010>:12: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n  \"weekofyear\": s.dt.weekofyear.values}\n"
     ]
    }
   ],
   "source": [
    "# Time series Data\n",
    "\n",
    "import pandas as pd\n",
    "# create a series of datetime with a frequency of 10 hours\n",
    "s = pd.date_range('2020-01-06', '2020-01-10', freq='10H').to_series()\n",
    "# create some features based on datetime\n",
    "features = {\"dayofweek\": s.dt.dayofweek.values,\n",
    "            \"dayofyear\": s.dt.dayofyear.values,\n",
    "            \"hour\": s.dt.hour.values,\n",
    "            \"is_leap_year\": s.dt.is_leap_year.values,\n",
    "            \"quarter\": s.dt.quarter.values,\n",
    "            \"weekofyear\": s.dt.weekofyear.values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'dayofweek': array([0, 0, 0, 1, 1, 2, 2, 2, 3, 3], dtype=int64),\n",
       " 'dayofyear': array([6, 6, 6, 7, 7, 8, 8, 8, 9, 9], dtype=int64),\n",
       " 'hour': array([ 0, 10, 20,  6, 16,  2, 12, 22,  8, 18], dtype=int64),\n",
       " 'is_leap_year': array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True]),\n",
       " 'quarter': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64),\n",
       " 'weekofyear': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)}"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Customer level Features\n",
    "\n",
    "def generate_features(df):\n",
    "    # create a bunch of features using the date column\n",
    "    df.loc[:, 'year'] = df['date'].dt.year\n",
    "    df.loc[:, 'weekofyear'] = df['date'].dt.weekofyear\n",
    "    df.loc[:, 'month'] = df['date'].dt.month\n",
    "    df.loc[:, 'dayofweek'] = df['date'].dt.dayofweek\n",
    "    df.loc[:, 'weekend'] = (df['date'].dt.weekday >=5).astype(int)\n",
    "    # create an aggregate dictionary\n",
    "    aggs = {} # for aggregation by month, we calculate the# number of unique month values and also the mean\n",
    "    aggs['month'] = ['nunique', 'mean']\n",
    "    aggs['weekofyear'] = ['nunique', 'mean']\n",
    "    # we aggregate by num1 and calculate sum, max, min # and mean values of this column\n",
    "    aggs['num1'] = ['sum','max','min','mean']\n",
    "    # for customer_id, we calculate the total count\n",
    "    aggs['customer_id'] = ['size']\n",
    "    # again for customer_id, we calculate the total unique\n",
    "    aggs['customer_id'] = ['nunique']\n",
    "    # we group by customer_id and calculate the aggregates\n",
    "    agg_df = df.groupby('customer_id').agg(aggs)\n",
    "    agg_df = agg_df.reset_index()\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statistical Features\n",
    "\n",
    "import numpy as np\n",
    "feature_dict = {}\n",
    "# calculate mean\n",
    "feature_dict['mean'] = np.mean(x)\n",
    "# calculate max\n",
    "feature_dict['max'] = np.max(x)\n",
    "# calculate min\n",
    "feature_dict['min'] = np.min(x)\n",
    "# calculate standard deviation\n",
    "feature_dict['std'] = np.std(x)\n",
    "# calculate variance\n",
    "feature_dict['var'] = np.var(x)\n",
    "# peak-to-peak\n",
    "feature_dict['ptp'] = np.ptp(x)\n",
    "# percentile features\n",
    "feature_dict['percentile_10'] = np.percentile(x, 10)\n",
    "feature_dict['percentile_60'] = np.percentile(x, 60)\n",
    "feature_dict['percentile_90'] = np.percentile(x, 90)\n",
    "# quantile features\n",
    "feature_dict['quantile_5'] = np.quantile(x,0.05)\n",
    "feature_dict['quantile_95'] = np.quantile(x, 0.95)\n",
    "feature_dict['quantile_99'] = np.quantile(x, 0.99)\n",
    "\n",
    "from tsfresh.feature_extraction import feature_calculators as fc\n",
    "# tsfresh based features\n",
    "feature_dict['abs_energy'] = fc.abs_energy(x)\n",
    "feature_dict['count_above_mean'] = fc.count_above_mean(x)\n",
    "feature_dict['count_below_mean'] = fc.count_below_mean(x)\n",
    "feature_dict['mean_abs_change'] = fc.mean_abs_change(x)\n",
    "feature_dict['mean_change'] = fc.mean_change(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Features\n",
    "import numpy as np\n",
    "# generate a random dataframe with # 2 columns and 100 rows\n",
    "df = pd.DataFrame(np.random.rand(100, 2),columns=[f\"f_{i}\"for i in range(1, 3)])\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "# initialize polynomial features class object\n",
    "# for two-degree polynomial features\n",
    "pf = preprocessing.PolynomialFeatures(degree=2,interaction_only=False,include_bias=False)\n",
    "# fit to the features\n",
    "pf.fit(df)\n",
    "# create polynomial features\n",
    "poly_feats = pf.transform(df)\n",
    "# create a dataframe with all the features\n",
    "num_feats = poly_feats.shape[1]\n",
    "df_transformed = pd.DataFrame(poly_feats,columns=[f\"f_{i}\"for i in range(1, num_feats + 1)])\n",
    "df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning\n",
    "# create bins of the numerical columns\n",
    "# 10 bins\n",
    "df[\"f_bin_10\"] = pd.cut(df[\"f_1\"], bins=10, labels=False)\n",
    "# 100 bins\n",
    "df[\"f_bin_100\"] = pd.cut(df[\"f_1\"], bins=100, labels=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(824.3364646464647, 0.8609664687403571)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Log Transformation (To reduce variance due to outliers)\n",
    "df.f_bin_100.var(), df.f_bin_100.apply(lambda x: np.log(1+ x)).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 6. ,  4. ,  9. ,  3. ,  5. ,  5. ],\n",
       "       [11. , 12.5, 13. ,  5. , 13. ,  2.5],\n",
       "       [ 5. ,  9. ,  1. ,  8. ,  9. ,  8. ],\n",
       "       [10. ,  8.5, 10. ,  8. , 10. ,  4. ],\n",
       "       [ 2. ,  6. ,  5. ,  8. ,  2. ,  6. ],\n",
       "       [12. ,  5. , 10. ,  9. , 10. ,  4.5],\n",
       "       [ 5. , 12. ,  2. ,  9. ,  2. ,  5. ],\n",
       "       [11. , 13. , 10. ,  3. ,  9. ,  1. ],\n",
       "       [12. , 12. , 10. ,  6. ,  9.5,  2.5],\n",
       "       [ 4. ,  5. ,  5. , 10. ,  3. ,  7. ]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "## Handling Missing Values\n",
    "## KNN Imputer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import impute\n",
    "# create a random numpy array with 10 samples\n",
    "# and 6 features and values ranging from 1 to 15\n",
    "X = np.random.randint(1, 15, (10, 6))\n",
    "# convert the array to float\n",
    "X = X.astype(float)\n",
    "# randomly assign 10 elements to NaN (missing)\n",
    "X.ravel()[np.random.choice(X.size, 10, replace=False)] = np.nan\n",
    "# use 2nearest neighbours to fill na values\n",
    "knn_imputer = impute.KNNImputer(n_neighbors=2)\n",
    "knn_imputer.fit_transform(X)\n",
    "\n"
   ]
  }
 ]
}